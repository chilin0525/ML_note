{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2021-08-18 11:18:34--  https://gist.githubusercontent.com/yrevar/942d3a0ac09ec9e5eb3a/raw/238f720ff059c1f82f368259d1ca4ffa5dd8f9f5/imagenet1000_clsidx_to_labels.txt\n",
      "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 30564 (30K) [text/plain]\n",
      "Saving to: ‘ImageNet_label’\n",
      "\n",
      "     0K .......... .......... .........                       100% 3.42M=0.009s\n",
      "\n",
      "2021-08-18 11:18:34 (3.42 MB/s) - ‘ImageNet_label’ saved [30564/30564]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "install ImageNet label and store into list\n",
    "\"\"\"\n",
    "import os\n",
    "os.system(\"wget -O ImageNet_label https://gist.githubusercontent.com/yrevar/942d3a0ac09ec9e5eb3a/raw/238f720ff059c1f82f368259d1ca4ffa5dd8f9f5/imagenet1000_clsidx_to_labels.txt\")\n",
    "\n",
    "label = []\n",
    "with open(\"ImageNet_label\") as f:\n",
    "    label = [i.split(\"'\")[1] for i in f.readlines()]\n",
    "\n",
    "# print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class timer for recording exe time\n",
    "\"\"\"\n",
    "import time\n",
    "class timer():\n",
    "    def __init__(self):\n",
    "        self.startTime=0\n",
    "        self.endTime=0\n",
    "        self.totaltime=0\n",
    "    def start(self):\n",
    "        self.startTime=time.time()\n",
    "    def end(self):\n",
    "        self.totaltime =time.time()-self.startTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "img transform \n",
    "\"\"\"\n",
    "from torchvision import transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "\t\tstd=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "save pytorch resnet model\n",
    "\"\"\"\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "resnet50 = torchvision.models.resnet50(pretrained=True, progress=False).eval()\n",
    "#print(resnet50)\n",
    "torch.save(resnet50,\"resnet.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load pytorch model: 0.039026 s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "load pytorch model\n",
    "\"\"\"\n",
    "\n",
    "load_torch_timer = timer()\n",
    "load_torch_timer.start()\n",
    "resnet_pytorch = torch.load(\"resnet.pt\")\n",
    "load_torch_timer.end()\n",
    "print(\"load pytorch model: %f s\" % (load_torch_timer.totaltime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "img = Image.open('cat.png')\n",
    "img = transform(img)\n",
    "img = torch.unsqueeze(img, 0)\n",
    "print(img.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n",
      "torch.Size([1000])\n",
      "pytorch time: 0.044211s\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "pytorch inference \n",
    "\"\"\"\n",
    "\n",
    "infer_torch_timer = timer()\n",
    "infer_torch_timer.start()\n",
    "output = resnet_pytorch(img)\n",
    "infer_torch_timer.end()\n",
    "\n",
    "print(output.size())\n",
    "output = output.squeeze()\n",
    "print(output.size())\n",
    "print(\"pytorch time: %fs\"%(infer_torch_timer.totaltime))\n",
    "\n",
    "sum = 0\n",
    "for i in output:\n",
    "    sum += i\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tabby, tabby cat          | 43.14913%\n",
      "tiger cat                 | 29.68253%\n",
      "Egyptian cat              | 25.96065%\n",
      "tiger, Panthera tigris    | 00.34236%\n",
      "lynx, catamount           | 00.30307%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "pytorch result\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn.functional as F\n",
    "_, index = torch.sort(output, descending=True)\n",
    "percentage = F.softmax(output)\n",
    "\n",
    "result_torch = [(label[i],percentage[i].item()*100) for i in index[:5]]\n",
    "for j in result_torch:\n",
    "    print(\"%-25s | %08.5f\"%(j[0],j[1])+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n)"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "load Pytorch model to GPU\n",
    "\"\"\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet_pytorch_gpu = torch.load(\"resnet.pt\")\n",
    "resnet_pytorch_gpu.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "pytorch time: 0.035235s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pytorch inference on GPU\n",
    "\"\"\"\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "img = Image.open('cat.png')\n",
    "img = transform(img)\n",
    "img = torch.unsqueeze(img, 0)\n",
    "print(img.size())\n",
    "\n",
    "infer_torch_gpu_timer = timer()\n",
    "infer_torch_gpu_timer.start()\n",
    "img.to(device)\n",
    "output = resnet_pytorch(img)\n",
    "infer_torch_gpu_timer.end()\n",
    "\n",
    "output = output.squeeze()\n",
    "print(\"pytorch time: %fs\"%(infer_torch_gpu_timer.totaltime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tabby, tabby cat          | 43.14913%\n",
      "tiger cat                 | 29.68253%\n",
      "Egyptian cat              | 25.96065%\n",
      "tiger, Panthera tigris    | 00.34236%\n",
      "lynx, catamount           | 00.30307%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "result of Pytorch on PGU\n",
    "\"\"\"\n",
    "\n",
    "_, index = torch.sort(output, descending=True)\n",
    "percentage = F.softmax(output)\n",
    "\n",
    "result_torch_gpu = [(label[i],percentage[i].item()*100) for i in index[:5]]\n",
    "for j in result_torch_gpu:\n",
    "    print(\"%-25s | %08.5f\"%(j[0],j[1])+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "image process and transform\n",
    "\"\"\"\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "img = Image.open('cat.png')\n",
    "img = transform(img)\n",
    "img = torch.unsqueeze(img, 0)\n",
    "print(img.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "pytorch to onnx and save as \"resnet.onnx\"\n",
    "\"\"\"\n",
    "\n",
    "import torch.onnx\n",
    "BATCH_SIZE=1\n",
    "dummy_input=torch.randn(BATCH_SIZE, 3, 224, 224)\n",
    "torch.onnx.export(resnet50, dummy_input, \"resnet.onnx\", verbose=False,input_names = ['input'], output_names = ['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n",
      "load pytorch model: 0.061211 s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "load onnx model\n",
    "\"\"\"\n",
    "\n",
    "import onnxruntime as rt\n",
    "\n",
    "\"\"\"\n",
    "Log severity level for a particular Run() invocation. \n",
    "0:Verbose, \n",
    "1:Info, \n",
    "2:Warning. \n",
    "3:Error, \n",
    "4:Fatal. \n",
    "\n",
    "Default is 2.\n",
    "\"\"\"\n",
    "rt.set_default_logger_severity(2)\n",
    "\n",
    "print(rt.get_device())\n",
    "load_onnx_timer = timer()\n",
    "load_onnx_timer.start()\n",
    "resnet_onnx = rt.InferenceSession(\"resnet.onnx\")\n",
    "load_onnx_timer.end()\n",
    "print(\"load pytorch model: %f s\" % (load_onnx_timer.totaltime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "onnx inference \n",
    "\"\"\"\n",
    "\n",
    "def to_np(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "infer_onnx_timer = timer()\n",
    "ort_input = {resnet_onnx.get_inputs()[0].name:to_np(img)}\n",
    "infer_onnx_timer.start()\n",
    "output = resnet_onnx.run(None,ort_input)[0][0]\n",
    "infer_onnx_timer.end()\n",
    "\n",
    "output = torch.tensor(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([281, 282, 285, 292, 287])\n",
      "tabby, tabby cat          | 43.14909%\n",
      "tiger cat                 | 29.68262%\n",
      "Egyptian cat              | 25.96060%\n",
      "tiger, Panthera tigris    | 00.34236%\n",
      "lynx, catamount           | 00.30307%\n"
     ]
    }
   ],
   "source": [
    "_, index = torch.sort(output, descending=True)\n",
    "percentage = F.softmax(output)\n",
    "\n",
    "print(index[:5])\n",
    "result_onnx = [(label[i],percentage[i].item()*100) for i in index[:5]]\n",
    "for j in result_onnx:\n",
    "    print(\"%-25s | %08.5f\"%(j[0],j[1])+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load time\n",
      "Pytorch : 0.039026s\n",
      "onnx    : 0.061211s\n",
      "--------------------------------------------------\n",
      "infer time\n",
      "Pytorch : 0.044211s\n",
      "onnx    : 0.095819s\n"
     ]
    }
   ],
   "source": [
    "print(\"load time\")\n",
    "print(\"Pytorch : %fs\" % (load_torch_timer.totaltime))\n",
    "print(\"onnx    : %fs\" % (load_onnx_timer.totaltime))\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"infer time\")\n",
    "print(\"Pytorch : %fs\" % (infer_torch_timer.totaltime))\n",
    "print(\"onnx    : %fs\" % (infer_onnx_timer.totaltime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.2.3.4\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "\n",
    "print(trt.__version__)\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "trt_runtime = trt.Runtime(TRT_LOGGER)\n",
    "\n",
    "def build_engine(onnx_path, shape = [1,224,224,3]):\n",
    "\n",
    "   \"\"\"\n",
    "   create the TensorRT engine\n",
    "   Args:\n",
    "      onnx_path : Path to onnx_file. \n",
    "      shape : Shape of the input of the ONNX file. \n",
    "  \"\"\"\n",
    "   with trt.Builder(TRT_LOGGER) as builder, builder.create_network(1) as network, trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "       builder.max_workspace_size = (256 << 20)\n",
    "       \"\"\"256 MB\"\"\"\n",
    "       with open(onnx_path, 'rb') as model:\n",
    "           parser.parse(model.read())\n",
    "       network.get_input(0).shape = shape\n",
    "       engine = builder.build_cuda_engine(network)\n",
    "       return engine\n",
    "\n",
    "def save_engine(engine, file_name):\n",
    "   buf = engine.serialize()\n",
    "   with open(file_name, 'wb') as f:\n",
    "       f.write(buf)\n",
    "\n",
    "def load_engine(trt_runtime, plan_path):\n",
    "   with open(plan_path, 'rb') as f:\n",
    "       engine_data = f.read()\n",
    "   engine = trt_runtime.deserialize_cuda_engine(engine_data)\n",
    "   return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx import ModelProto\n",
    "\n",
    "BATCH_SIZE=1\n",
    "dummy_input=torch.randn(BATCH_SIZE, 3, 224, 224)\n",
    "resnet50 = torchvision.models.resnet50(pretrained=True, progress=False).eval()\n",
    "torch.onnx.export(resnet50, dummy_input, \"resnet.onnx\", verbose=False,input_names = ['input'], output_names = ['output'])\n",
    "\n",
    "onnx_path = \"resnet.onnx\"\n",
    "trt_path  = \"resnet.plan\"\n",
    "\n",
    "model = ModelProto()\n",
    "with open(onnx_path, \"rb\") as f:\n",
    "  model.ParseFromString(f.read())\n",
    "\n",
    "batch_size = 1\n",
    "d0 = model.graph.input[0].type.tensor_type.shape.dim[1].dim_value\n",
    "d1 = model.graph.input[0].type.tensor_type.shape.dim[2].dim_value\n",
    "d2 = model.graph.input[0].type.tensor_type.shape.dim[3].dim_value\n",
    "shape = [batch_size , d0, d1 ,d2]\n",
    "\n",
    "\"\"\"\n",
    "build engine and start timer to get total build time\n",
    "\"\"\"\n",
    "build_trt_timer = timer()\n",
    "build_trt_timer.start()\n",
    "trt_engine = build_engine(onnx_path,shape)\n",
    "build_trt_timer.end()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "save engine as trt_path\n",
    "\"\"\"\n",
    "save_engine(trt_engine,trt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build a tensorRT engine: 8.927228 s\n"
     ]
    }
   ],
   "source": [
    "print(\"build a tensorRT engine: %f s\" %(build_trt_timer.totaltime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit \n",
    "\n",
    "def allocate_buffers(engine, batch_size, data_type):\n",
    "   \"\"\"\n",
    "   This is the function to allocate buffers for input and output in the device\n",
    "   args:\n",
    "      engine      : The path to the TensorRT engine. \n",
    "      batch_size  : The batch size for execution time.\n",
    "      data_type   : The type of the data for input and output, for example trt.float32. \n",
    "   \n",
    "   return:\n",
    "      h_input_1   : Input in the host.\n",
    "      d_input_1   : Input in the device. \n",
    "      h_output_1  : Output in the host. \n",
    "      d_output_1  : Output in the device. \n",
    "      stream      : CUDA stream.\n",
    "   \"\"\"\n",
    "\n",
    "   # Determine dimensions and create page-locked memory buffers (which won't be swapped to disk) to hold host inputs/outputs.\n",
    "   h_input_1 = cuda.pagelocked_empty(batch_size * trt.volume(engine.get_binding_shape(0)), dtype=trt.nptype(data_type))\n",
    "   h_output = cuda.pagelocked_empty(batch_size * trt.volume(engine.get_binding_shape(1)), dtype=trt.nptype(data_type))\n",
    "   # Allocate device memory for inputs and outputs.\n",
    "   d_input_1 = cuda.mem_alloc(h_input_1.nbytes)\n",
    "\n",
    "   d_output = cuda.mem_alloc(h_output.nbytes)\n",
    "   # Create a stream in which to copy inputs/outputs and run inference.\n",
    "   stream = cuda.Stream()\n",
    "   return h_input_1, d_input_1, h_output, d_output, stream \n",
    "\n",
    "def load_images_to_buffer(pics, pagelocked_buffer):\n",
    "   preprocessed = np.asarray(pics).ravel()\n",
    "   np.copyto(pagelocked_buffer, preprocessed) \n",
    "\n",
    "def do_inference(engine, pics_1, h_input_1, d_input_1, h_output, d_output, stream, batch_size):\n",
    "   \"\"\"\n",
    "   This is the function to run the inference\n",
    "   Args:\n",
    "      engine      : Path to the TensorRT engine \n",
    "      pics_1      : Input images to the model.  \n",
    "      h_input_1   : Input in the host         \n",
    "      d_input_1   : Input in the device \n",
    "      h_output_1  : Output in the host \n",
    "      d_output_1  : Output in the device \n",
    "      stream      : CUDA stream\n",
    "      batch_size  : Batch size for execution time\n",
    "   \n",
    "   Output:\n",
    "      The list of output images\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "   load_images_to_buffer(pics_1, h_input_1)\n",
    "\n",
    "   with engine.create_execution_context() as context:\n",
    "\n",
    "      \"\"\" copy data from host to device \"\"\"\n",
    "      cuda.memcpy_htod_async(d_input_1, h_input_1, stream)\n",
    "\n",
    "      # Run inference.\n",
    "      context.profiler = trt.Profiler()\n",
    "      context.execute(batch_size=1, bindings=[int(d_input_1), int(d_output)])\n",
    "\n",
    "      # Transfer predictions back from the GPU.\n",
    "      cuda.memcpy_dtoh_async(h_output, d_output, stream)\n",
    "\n",
    "      # Synchronize the stream\n",
    "      stream.synchronize()\n",
    "      \n",
    "      # Return the host output.\n",
    "      \n",
    "      return h_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "load tensorRT engine\n",
    "\"\"\"\n",
    "\n",
    "load_trt_timer = timer()\n",
    "load_trt_timer.start()\n",
    "resnet_trt = load_engine(trt_runtime,trt_path)\n",
    "load_trt_timer.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load a tensorRT engine: 0.507418 s\n"
     ]
    }
   ],
   "source": [
    "print(\"load a tensorRT engine: %f s\" %(load_trt_timer.totaltime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "inference step\n",
    "   workflow:\n",
    "      1. allocate buffer for input and output in GPU\n",
    "      2. copy data from host to device\n",
    "      3. run inference in GPU\n",
    "      4. copy result from GPU to the host\n",
    "      5. reshape (optional)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" allocate buffer \"\"\"\n",
    "\n",
    "host_input, device_input, host_output, device_output, stream = allocate_buffers(\n",
    "    engine = resnet_trt,\n",
    "    batch_size = 1,\n",
    "    data_type = trt.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "img = Image.open('cat.png')\n",
    "img = transform(img)\n",
    "img = torch.unsqueeze(img, 0)\n",
    "print(img.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv_0 + Relu_1: 0.08096ms\n",
      "MaxPool_2: 0.017728ms\n",
      "Conv_3 + Relu_4: 0.019552ms\n",
      "Conv_5 + Relu_6: 0.050528ms\n",
      "Conv_7: 0.0408ms\n",
      "Conv_8 + Add_9 + Relu_10: 0.046816ms\n",
      "Conv_11 + Relu_12: 0.043648ms\n",
      "Conv_13 + Relu_14: 0.049952ms\n",
      "Conv_15 + Add_16 + Relu_17: 0.046208ms\n",
      "Conv_18 + Relu_19: 0.043552ms\n",
      "Conv_20 + Relu_21: 0.05072ms\n",
      "Conv_22 + Add_23 + Relu_24: 0.046048ms\n",
      "Conv_22 + Add_23 + Relu_24 output reformatter 0: 0.030784ms\n",
      "Conv_25 + Relu_26: 0.063904ms\n",
      "Conv_27 + Relu_28: 0.073728ms\n",
      "Conv_29: 0.036672ms\n",
      "Conv_30 + Add_31 + Relu_32: 0.069056ms\n",
      "Conv_33 + Relu_34: 0.038528ms\n",
      "Conv_35 + Relu_36: 0.0736ms\n",
      "Conv_37 + Add_38 + Relu_39: 0.042816ms\n",
      "Conv_40 + Relu_41: 0.038368ms\n",
      "Conv_42 + Relu_43: 0.073536ms\n",
      "Conv_44 + Add_45 + Relu_46: 0.04272ms\n",
      "Conv_47 + Relu_48: 0.038208ms\n",
      "Conv_49 + Relu_50: 0.074272ms\n",
      "Conv_51 + Add_52 + Relu_53: 0.04304ms\n",
      "Conv_54 + Relu_55: 0.061504ms\n",
      "Conv_56 + Relu_57: 0.134048ms\n",
      "Conv_58: 0.061344ms\n",
      "Conv_59 + Add_60 + Relu_61: 0.108416ms\n",
      "Conv_59 + Add_60 + Relu_61 output reformatter 0: 0.011712ms\n",
      "Conv_62 + Relu_63: 0.05712ms\n",
      "Conv_64 + Relu_65: 0.076192ms\n",
      "Conv_66 + Add_67 + Relu_68: 0.060896ms\n",
      "Conv_69 + Relu_70: 0.056576ms\n",
      "Conv_71 + Relu_72: 0.076096ms\n",
      "Conv_73 + Add_74 + Relu_75: 0.061312ms\n",
      "Conv_76 + Relu_77: 0.057568ms\n",
      "Conv_78 + Relu_79: 0.076704ms\n",
      "Conv_80 + Add_81 + Relu_82: 0.061728ms\n",
      "Conv_83 + Relu_84: 0.056448ms\n",
      "Conv_85 + Relu_86: 0.076192ms\n",
      "Conv_87 + Add_88 + Relu_89: 0.06032ms\n",
      "Conv_90 + Relu_91: 0.057344ms\n",
      "Conv_92 + Relu_93: 0.076192ms\n",
      "Conv_94 + Add_95 + Relu_96: 0.061024ms\n",
      "Conv_97 + Relu_98: 0.093664ms\n",
      "Conv_99 + Relu_100 input reformatter 0: 0.00848ms\n",
      "Conv_99 + Relu_100: 0.256128ms\n",
      "Conv_101 input reformatter 0: 0.006944ms\n",
      "Conv_101: 0.057824ms\n",
      "Conv_102 + Add_103 + Relu_104: 0.208928ms\n",
      "Conv_105 + Relu_106: 0.095936ms\n",
      "Conv_107 + Relu_108: 0.103552ms\n",
      "Conv_109 + Add_110 + Relu_111: 0.10384ms\n",
      "Conv_112 + Relu_113: 0.09488ms\n",
      "Conv_114 + Relu_115: 0.1032ms\n",
      "Conv_116 + Add_117 + Relu_118: 0.10224ms\n",
      "GlobalAveragePool_119: 0.008736ms\n",
      "Gemm_121: 0.036608ms\n"
     ]
    }
   ],
   "source": [
    "\"\"\" inference \"\"\"\n",
    "infer_trt_timer = timer()\n",
    "infer_trt_timer.start()\n",
    "output = do_inference(\n",
    "    engine    = resnet_trt,\n",
    "    pics_1    = img,\n",
    "    h_input_1 = host_input,\n",
    "    d_input_1 = device_input,\n",
    "    h_output  = host_output,\n",
    "    d_output  = device_output,\n",
    "    stream    = stream,\n",
    "    batch_size= 1,\n",
    ")\n",
    "infer_trt_timer.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(output))\n",
    "\"\"\" transform ndarray to Tensor \"\"\"\n",
    "output = torch.tensor(output)\n",
    "print(type(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([281, 282, 285, 292, 287])\n",
      "tabby, tabby cat          | 43.14908%\n",
      "tiger cat                 | 29.68258%\n",
      "Egyptian cat              | 25.96064%\n",
      "tiger, Panthera tigris    | 00.34236%\n",
      "lynx, catamount           | 00.30307%\n"
     ]
    }
   ],
   "source": [
    "_, index = torch.sort(output, descending=True)\n",
    "percentage = F.softmax(output)\n",
    "\n",
    "print(index[:5])\n",
    "result_trt = [(label[i],percentage[i].item()*100) for i in index[:5]]\n",
    "for j in result_trt:\n",
    "    print(\"%-25s | %08.5f\"%(j[0],j[1])+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load time\n",
      "Pytorch      : 0.039026s\n",
      "onnx runtime : 0.061211s\n",
      "tensorRT     : 0.507418s\n",
      "--------------------------------------------------\n",
      "infer time\n",
      "Pytorch      : 0.044211s\n",
      "Pytorch(GPU) : 0.035235s\n",
      "onnx runtime : 0.095819s\n",
      "tensorRT     : 0.009560s\n",
      "--------------------------------------------------\n",
      "result\n",
      "  Pytorch:\n",
      "     tabby, tabby cat          | 43.14913%\n",
      "     tiger cat                 | 29.68253%\n",
      "     Egyptian cat              | 25.96065%\n",
      "     tiger, Panthera tigris    | 00.34236%\n",
      "     lynx, catamount           | 00.30307%\n",
      "  Pytorch(GPU):\n",
      "     tabby, tabby cat          | 43.14913%\n",
      "     tiger cat                 | 29.68253%\n",
      "     Egyptian cat              | 25.96065%\n",
      "     tiger, Panthera tigris    | 00.34236%\n",
      "     lynx, catamount           | 00.30307%\n",
      "     ---------------------------------------------\n",
      "  onnx:\n",
      "     tabby, tabby cat          | 43.14909%\n",
      "     tiger cat                 | 29.68262%\n",
      "     Egyptian cat              | 25.96060%\n",
      "     tiger, Panthera tigris    | 00.34236%\n",
      "     lynx, catamount           | 00.30307%\n",
      "     ---------------------------------------------\n",
      "  tensorRT:\n",
      "     tabby, tabby cat          | 43.14908%\n",
      "     tiger cat                 | 29.68258%\n",
      "     Egyptian cat              | 25.96064%\n",
      "     tiger, Panthera tigris    | 00.34236%\n",
      "     lynx, catamount           | 00.30307%\n"
     ]
    }
   ],
   "source": [
    "print(\"load time\")\n",
    "print(\"Pytorch      : %fs\" % (load_torch_timer.totaltime))\n",
    "print(\"onnx runtime : %fs\" % (load_onnx_timer.totaltime))\n",
    "print(\"tensorRT     : %fs\" % (load_trt_timer.totaltime))\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"infer time\")\n",
    "print(\"Pytorch      : %fs\" % (infer_torch_timer.totaltime))\n",
    "print(\"Pytorch(GPU) : %fs\" % (infer_torch_gpu_timer.totaltime))\n",
    "print(\"onnx runtime : %fs\" % (infer_onnx_timer.totaltime))\n",
    "print(\"tensorRT     : %fs\" % (infer_trt_timer.totaltime))\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"result\")\n",
    "print(\"  Pytorch:\")\n",
    "for j in result_torch:\n",
    "    print(\"     %-25s | %08.5f\"%(j[0],j[1])+\"%\")\n",
    "\n",
    "print(\"  Pytorch(GPU):\")\n",
    "for j in result_torch_gpu:\n",
    "    print(\"     %-25s | %08.5f\"%(j[0],j[1])+\"%\")\n",
    "\n",
    "print(\" \"*5+\"-\"*45)\n",
    "print(\"  onnx:\")\n",
    "for j in result_onnx:\n",
    "    print(\"     %-25s | %08.5f\"%(j[0],j[1])+\"%\")\n",
    "\n",
    "print(\" \"*5+\"-\"*45)\n",
    "print(\"  tensorRT:\")\n",
    "for j in result_trt:\n",
    "    print(\"     %-25s | %08.5f\"%(j[0],j[1])+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArwAAAIHCAYAAAB9t4/1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAtXElEQVR4nO3debgsVX3v//cHEBwYHJhUUBzQqJjIcDU4RBRJUBMlDoFonPgZLyoKERPBCRQUDAbRkMQJEaJBJD9FEwx4iYIDXBAUREDF4ShDDogDR2RS+N4/qlqbZvfZw+l9ep913q/n6WfvWrWqalV3796fXr1qdaoKSZIkqVXrTLsBkiRJ0mIy8EqSJKlpBl5JkiQ1zcArSZKkphl4JUmS1DQDryRJkppm4JUkSVLTDLySJElqmoFXkiRJTTPwSlqrJXlpkkqyzQK33zbJ55Nc3+9nj8m2sA1Jzkxy5rTbIWnttN60GyBJa7jjgQcBbwJ+AZw/1dZMUZJHAn8BfLSqlk25OZL0W6mqabdBkqYmybrAXYBbap4viEnuBtwIvKOq3rwY7VuTJHkecDLwlKo6c2Td+gBVdesUmiZpLWcPr6S1WlXdBty2wM0363/+YjKtgSR3BW6tqtsntc+lwKAraZocwytprTbTGN4ky5L8Z5InJjkvyc1JfpDkxUN1DgF+1C8e2e9j2dD6+yf5SJJrktyS5JIke48ce5d+u72SHJbkKroe44379Y9Lclo/PvjGJGclecLIPg7p9/HQJB9N8ou+/nFJ7j7D+f5Vf043Jvl5ki8l+eOROk9P8uUkv0ryyySnJnnUbPcjXe8uwBf7NlWSXfr1dxjDO3Tuf5Hk4CRX9cf69ySbJNkgydFJrk1yQ38+G4w5nwuS3JTkZ0k+kWTrlbVV0trHHl5JmtlDgX8HjqUbp7s38NEkF1TVJcCn6Hp23wOcCHwOuAEgyRbA/wUKOAb4CfB04NgkG1fV0SPHegtwK/BuYAPg1iRPBf4LuAB4G3A78DLgC0meVFXnjezjk8APgYOAHYCXA9cCbxhUSHIwcAhwNvDW/piPA54KfL6v86L+fE/vt7078ErgK0m2X8nY3C8B7wNeC7wTuKwvv2xM/YGDgJuAI+ju89cAv+7P9159e/8QeGl/fm8fOp83AYf25/5huh731wBf6tv6i1mOLWltUVXevHnzttbe6IJUAdsMlS3ry540VLYZcDPw7qGybfp6rx/Z54eBq4H7jJSfSBeS79Yv79Jv//1BWV8e4LvAafTXWvTldwN+AHx+qOyQfh/HjhzrU8B1Q8sPpRu68SlgnZG6g+s5NgR+DnxwZP0WfbvvUD7Dffm8vi27zLDuTODMoeXBuV8M3GWo/N/owu7nRrY/G1g2tPxA4DfAG0fqbUcXmN+4srZ68+Zt7bo5pEGSZnZpVX15sFBVPwG+Azx4ZRslCfBc4D/6xU0HN7pe003oemCHHV9VNw0tPwbYli783Wdo+3sA/w38UZLR1+/3jyx/ud924355D7phbG+vkfHBVTW4WG834J7AiSPtvg04F3jKys59gU6oql8PLZ9LF/g/MlLvXGDrJINPJp9Ddz6fHGnrcuDyRWqrpDWUQxokaWY/nqHs53Qfs6/MZnSh8RX9bSabjyz/cGR52/7n8Ss5ziZ9ewZG2ztYdy9gBfAQup7TS1eyz8FxvzBm/YqVbLtQo+2+vv95xQzl69Cd90/p2hq6cDuTX48pl7QWMvBK0szGzdyQWbYb9Lx+jPGB9ZsjyzeNLA/28bfAhWP2ccPI8kLbO9NxX0TXUzrqN/PY11yNa/ds57MO3ZCIp4+pO3r/SFqLGXglabJ+AvwSWLeqzljgPr7f/1yxCvuYaZ/rAI9kfIgeHPfaBR53dU7s/n268PvDqvruajyupDWQY3glaYKqm9f3/weem2S70fVJNrvzVndyAV2ge32SDRe4j1Gn0A1peOvo+N9+3DF0Y4xXAG9McpcFHPdX/c97LqB98/Upup7dg4faD3Tnk+Q+q6ENktYQ9vBK0uQdSHfR1LlJPkQ3bvbedBerPa3/fayquj3Jy+mmJbskyXHAVcD9+/2uAP5sPg2qqu8leQfdFGhfTvIp4Bbgf9HNKHFQVa1I8krgX4GvJ/kEXY/1A4BnAl8F9l3JYS6kC6FvSLJJv/8vVNW182nrHM/n+0neDBwObJPkFLqe9QcBfw58kG6aN0ky8ErSpFXVNUkeSzfX7XOAV9FdaHUJQ/PizrKPM5PsTBdQ96WbMmw53WwFH1hgu96a5Id0c9W+g+5LLr5JF3AHdf4tydV0of1v6eYFvopu1ofjZtn/8iT70M2teyywLl1An3jg7Y93RJLvAn8DHNwXX0E3p/BnF+OYktZM+d1sNJIkSVJ7HMMrSZKkphl4JUmS1DQDryRJkppm4JUkSVLTDLySJElqmoFXkiRJTZv6PLxJXk031+OWwEXAa6rqvDF1HwW8HdgReCDwN1V19Krsc8xxAtyPbhJzSZIkLU0bAVfXLPPsTjXwJtkTOArYh24y9f2B05M8fMw389wd+AFwMvCeCe1zJvcDrpz7mUiSJGlKtqL7gpyxpvrFE0nOBb5WVfv2y+vQfUvOP1bVEbNsuww4erSHd1X2ObSPjYHrr7jiCjbeeOP5nZQkSZIW3YoVK9h6660BNqmqFSurO7Ue3iTr0w1NOHxQ1n9//BnAzqtzn0k2oPv6zIGNADbeeGMDryRJ0hpumhetbUr3PevXjJRfQzf2dnXu8yDg+qGbwxkkSZIa4SwNncOBTYZuW023OZIkSZqUaV60dh1wG7DFSPkWwPLVuc+qugW4ZbDcTdIgSZKkFkyth7eqbgUuAHYdlPUXmO0KnLNU9ilJkqQ127Tn4T0KOD7J+cB5dFOI3QM4DiDJCcBVVXVQv7w+8Mh+2/WB+yd5DHBDVX1vLvuUJEnS2mWqgbeqTkqyGd2XSWwJXAjsXlWDi84eANw+tMn9gG8MLb++v50F7DLHfUqSJGktMtV5eJeqwTy8119/vdOSSZIkLUErVqxgk002gTnMw+ssDZIkSWqagVeSJElNM/BKkiSpaQZeSZIkNc3AK0mSpKYZeCVJktQ0A68kSZKaZuCVJElS0wy8kiRJapqBV5IkSU0z8EqSJKlpBl5JkiQ1bb1pN0CStPRtc+Cp026CFsmyI5457SZIi84eXkmSJDXNwCtJkqSmGXglSZLUNAOvJEmSmmbglSRJUtMMvJIkSWqagVeSJElNM/BKkiSpaQZeSZIkNc3AK0mSpKYZeCVJktQ0A68kSZKaZuCVJElS0wy8kiRJapqBV5IkSU0z8EqSJKlpBl5JkiQ1zcArSZKkphl4JUmS1DQDryRJkppm4JUkSVLTDLySJElqmoFXkiRJTTPwSpIkqWkGXkmSJDXNwCtJkqSmGXglSZLUNAOvJEmSmmbglSRJUtMMvJIkSWqagVeSJElNM/BKkiSpaQZeSZIkNc3AK0mSpKYZeCVJktQ0A68kSZKaZuCVJElS0wy8kiRJapqBV5IkSU0z8EqSJKlpBl5JkiQ1zcArSZKkphl4JUmS1DQDryRJkppm4JUkSVLTDLySJElqmoFXkiRJTTPwSpIkqWkGXkmSJDXNwCtJkqSmGXglSZLUNAOvJEmSmmbglSRJUtMMvJIkSWqagVeSJElNM/BKkiSpaQZeSZIkNc3AK0mSpKYZeCVJktQ0A68kSZKaZuCVJElS0wy8kiRJapqBV5IkSU0z8EqSJKlpBl5JkiQ1zcArSZKkpk098CZ5dZJlSW5Ocm6Sx85S//lJvt3XvzjJM0bWb5jkmCRXJrkpyaVJ9lncs5AkSdJSNdXAm2RP4CjgbcAOwEXA6Uk2H1P/8cCJwLHA9sApwClJthuqdhSwO/BXwCOAo4Fjkjxrcc5CkiRJS9m0e3hfB3yoqo6rqkuBfYAbgb3H1N8POK2qjqyqy6rqLcDXgX2H6jweOL6qzqyqZVX1QbogvdKeY0mSJLVpaoE3yfrAjsAZg7Kqur1f3nnMZjsP1++dPlL/bOBZSe6fzlOAhwGfX0lbNkiy8eAGbDTvE5IkSdKSNM0e3k2BdYFrRsqvAbYcs82Wc6j/GuBS4ErgVuA04NVV9aWVtOUg4Pqh25VzaL8kSZLWANMe0rAYXgP8IfAsuh7kA4B/SvK0lWxzOLDJ0G2rxW6kJEmSVo/1pnjs64DbgC1GyrcAlo/ZZvnK6ie5G/BO4M+r6tR+/TeTPAZ4PXceDgFAVd0C3DJYTjLnk5AkSdLSNrUe3qq6FbgA2HVQlmSdfvmcMZudM1y/t9tQ/bv0t9tH6txGm73ZkiRJmsU0e3ihm0Ls+CTnA+cB+wP3AI4DSHICcFVVHdTXfy9wVpIDgFOBvYCdgFcAVNWKJGcBRya5CfgR8GTgxXQzQkiSJGktM9XAW1UnJdkMeDvdhWcXArtX1eDCtAcw1FtbVWcneQFwGN3QhcuBParqW0O73YtuTO7HgXvThd43Ae9f3LORJEnSUjTtHl6q6hjgmDHrdpmh7GTg5JXsbznwskm1T5IkSWs2x7VKkiSpaQZeSZIkNc3AK0mSpKYZeCVJktQ0A68kSZKaZuCVJElS0wy8kiRJapqBV5IkSU0z8EqSJKlpBl5JkiQ1zcArSZKkphl4JUmS1DQDryRJkppm4JUkSVLTDLySJElqmoFXkiRJTTPwSpIkqWkGXkmSJDXNwCtJkqSmGXglSZLUNAOvJEmSmmbglSRJUtMMvJIkSWqagVeSJElNM/BKkiSpaQZeSZIkNc3AK0mSpKYZeCVJktQ0A68kSZKaZuCVJElS0wy8kiRJapqBV5IkSU0z8EqSJKlpBl5JkiQ1zcArSZKkphl4JUmS1DQDryRJkppm4JUkSVLTDLySJElqmoFXkiRJTTPwSpIkqWkGXkmSJDXNwCtJkqSmGXglSZLUNAOvJEmSmmbglSRJUtMMvJIkSWqagVeSJElNM/BKkiSpaQZeSZIkNc3AK0mSpKYZeCVJktQ0A68kSZKaZuCVJElS0wy8kiRJapqBV5IkSU0z8EqSJKlpBl5JkiQ1zcArSZKkphl4JUmS1DQDryRJkppm4JUkSVLTDLySJElqmoFXkiRJTTPwSpIkqWkGXkmSJDXNwCtJkqSmGXglSZLUNAOvJEmSmmbglSRJUtMMvJIkSWqagVeSJElNM/BKkiSpaQZeSZIkNc3AK0mSpKYZeCVJktQ0A68kSZKaZuCVJElS0wy8kiRJatrUA2+SVydZluTmJOcmeews9Z+f5Nt9/YuTPGOGOo9I8tkk1yf5VZKvJXnA4p2FJEmSlqqpBt4kewJHAW8DdgAuAk5PsvmY+o8HTgSOBbYHTgFOSbLdUJ2HAF8Bvg3sAvw+cChw82KdhyRJkpauaffwvg74UFUdV1WXAvsANwJ7j6m/H3BaVR1ZVZdV1VuArwP7DtV5B/C5qvq7qvpGVX2/qj5bVdcu5olIkiRpaZpa4E2yPrAjcMagrKpu75d3HrPZzsP1e6cP6idZB3gm8N0kpye5th8msccsbdkgycaDG7DRQs5JkiRJS880e3g3BdYFrhkpvwbYcsw2W85Sf3NgQ+BA4DTgj4FPA59K8uSVtOUg4Pqh25VzOwVJkiQtddMe0jBpg/P5TFW9p6ourKojgP+kGy4xzuHAJkO3rRa3mZIkSVpd1pvisa8DbgO2GCnfAlg+Zpvls9S/DvgNcOlIncuAJ45rSFXdAtwyWE6ysnZLkiRpDTK1Ht6quhW4ANh1UNaPwd0VOGfMZucM1+/tNqjf7/NrwMNH6jwM+NGqt1qSJElrmmn28EI3JdnxSc4HzgP2B+4BHAeQ5ATgqqo6qK//XuCsJAcApwJ7ATsBrxja55HASUm+BHwR2B34M7opyiRJkrSWmWrgraqTkmwGvJ3uwrMLgd2ranBh2gOA24fqn53kBcBhwDuBy4E9qupbQ3U+nWQfugvR3gd8B3huVX1lNZySJEmSlphp9/BSVccAx4xZt8sMZScDJ8+yz48AH5lE+yRJkrRma22WBkmSJOkODLySJElqmoFXkiRJTTPwSpIkqWkGXkmSJDXNwCtJkqSmGXglSZLUNAOvJEmSmmbglSRJUtMMvJIkSWqagVeSJElNM/BKkiSpaestZKMkDwKeBDwQuDvwE+AbwDlVdfPkmidJkiStmnkF3iQvBPYDdgKuAa4GbgLuDTwEuDnJx4F3VdWPJtxWSZIkad7mHHiTfAO4Ffgo8NyqumJk/QbAzsBewPlJXlVVJ0+wrZIkSdK8zaeH98CqOn3cyqq6BTgTODPJm4BtVq1pkiRJ0qqbc+BdWdidoe5PgZ8uqEWSJEnSBC1oloYkOyR59NDys5OckuSdSdafXPMkSZKkVbPQack+ADwMIMmDgU8ANwLPB/5+Mk2TJEmSVt1CA+/DgAv7358PfKmqXgC8FHjuqjdLkiRJmoyFBt4Mbfs04HP971cAm65qoyRJkqRJWWjgPR94c5IXAU8GTu3LH0Q3P68kSZK0JCw08O4P7AAcA7yjqr7Xlz8POHsC7ZIkSZImYkFfLVxV3wQePcOqvwVuW6UWSZIkSRM0n29aS1XVyupU1c2r3iRJkiRpcuYzpOGSJHvNNs9ukm2T/EuSA1exbZIkSdIqm8+QhtcA7wL+Ocn/obtw7WrgZuBewCOBJwKPohvb+y+TbaokSZI0f/P5auH/BnZK8kRgT+CFwAOBuwHXAd8ATgA+XlU/X4S2SpIkSfM274vWquorwFcWoS2SJEnSxC10WjJJkiRpjWDglSRJUtMMvJIkSWqagVeSJElNM/BKkiSpaQsOvEkekuSwJCcm2bwve3qSR02ueZIkSdKqWVDgTfJk4GLgccBzgA37VX8AvG0yTZMkSZJW3UJ7eI8A3lxVuwG3DpV/AfjDVW6VJEmSNCELDbyPBj49Q/m1wKYLb44kSZI0WQsNvL8A7jtD+fbAVQtujSRJkjRhCw28nwDelWRLoIB1kjwBeDdwwqQaJ0mSJK2qhQbeNwLfBq6gu2DtUuBLwNnAYZNpmiRJkrTq1lvIRlV1K/DXSQ4FtqMLvd+oqssn2ThJkiRpVS0o8A5U1Y+BH0+oLZIkSdLELSjwJgnwPOApwOaMDI2oquesetMkSZKkVbfQHt6jgf8NfBG4hu7CNUmSJGnJWWjgfRHwnKr63CQbI0mSJE3aQmdpuB74wSQbIkmSJC2GhQbeQ4CDk9xtgm2RJEmSJm6hQxo+CfwlcG2SZcCvh1dW1Q6r2C5JkiRpIhYaeI8HdgQ+hhetSZIkaQlbaOB9JvAnVfWVSTZGkiRJmrSFjuG9AlgxyYZIkiRJi2GhgfcA4O+TbDPBtkiSJEkTt9AhDR8D7g58P8mN3PmitXuvasMkSZKkSVho4N1/ko2QJEmSFsuCAm9VHT/phkiSJEmLYc6BN8nGVbVi8PvK6g7qSZIkSdM2nx7enye5b1VdC/yCmefeTV++7gTaJkmSJK2y+QTepwI/639/yiK0RZIkSZq4OQfeqjpraPGHwBVVdYde3iQBtp5Q2yRJkqRVttB5eH8IbDZD+b37dZIkSdKSsNDAOxirO2pD4OaFN0eSJEmarHlNS5bkqP7XAg7tv3RiYF3gccCFk2maJEmStOrmOw/v9v3PAI8Gbh1adytwEfDuCbRLkiRJmoh5Bd6qegpAkuOA/Zxvd3K2OfDUaTdBi2TZEc+cdhMkSVqrLfSb1l426YZIkiRJi2GhF61JkiRJawQDryRJkppm4JUkSVLTDLySJElqmoFXkiRJTTPwSpIkqWkGXkmSJDXNwCtJkqSmGXglSZLUNAOvJEmSmmbglSRJUtMMvJIkSWqagVeSJElNWxKBN8mrkyxLcnOSc5M8dpb6z0/y7b7+xUmesZK6709SSfafeMMlSZK05E098CbZEzgKeBuwA3ARcHqSzcfUfzxwInAssD1wCnBKku1mqPvnwB8CVy9K4yVJkrTkTT3wAq8DPlRVx1XVpcA+wI3A3mPq7wecVlVHVtVlVfUW4OvAvsOVktwf+EfghcCvF631kiRJWtKmGniTrA/sCJwxKKuq2/vlncdstvNw/d7pw/WTrAP8K3BkVV0yh3ZskGTjwQ3YaF4nIkmSpCVr2j28mwLrAteMlF8DbDlmmy3nUP8NwG+A982xHQcB1w/drpzjdpIkSVriph14Jy7JjnTDHl5aVTXHzQ4HNhm6bbVIzZMkSdJqNu3Aex1wG7DFSPkWwPIx2yyfpf6TgM2BHyf5TZLfAA8E/iHJspl2WFW3VNWKwQ345bzPRJIkSUvSVANvVd0KXADsOijrx9/uCpwzZrNzhuv3dhuq/6/A7wOPGbpdDRwJ/MlEGi5JkqQ1xnrTbgDdlGTHJzkfOA/YH7gHcBxAkhOAq6rqoL7+e4GzkhwAnArsBewEvAKgqn4K/HT4AEl+DSyvqu8s+tlIkiRpSZl64K2qk5JsBryd7sKzC4Hdq2pwYdoDgNuH6p+d5AXAYcA7gcuBParqW6u14ZIkSVojTD3wAlTVMcAxY9btMkPZycDJ89j/NgttmyRJktZs075oTZIkSVpUBl5JkiQ1zcArSZKkphl4JUmS1DQDryRJkppm4JUkSVLTDLySJElqmoFXkiRJTTPwSpIkqWkGXkmSJDXNwCtJkqSmGXglSZLUNAOvJEmSmmbglSRJUtMMvJIkSWraetNugKTFsc2Bp067CVoEy4545rSbIElrHHt4JUmS1DQDryRJkppm4JUkSVLTDLySJElqmoFXkiRJTTPwSpIkqWkGXkmSJDXNwCtJkqSmGXglSZLUNAOvJEmSmmbglSRJUtMMvJIkSWqagVeSJElNM/BKkiSpaQZeSZIkNc3AK0mSpKYZeCVJktQ0A68kSZKaZuCVJElS0wy8kiRJapqBV5IkSU0z8EqSJKlpBl5JkiQ1zcArSZKkphl4JUmS1DQDryRJkppm4JUkSVLTDLySJElqmoFXkiRJTTPwSpIkqWkGXkmSJDXNwCtJkqSmGXglSZLUNAOvJEmSmmbglSRJUtMMvJIkSWqagVeSJElNM/BKkiSpaQZeSZIkNc3AK0mSpKYZeCVJktQ0A68kSZKaZuCVJElS0wy8kiRJapqBV5IkSU0z8EqSJKlpBl5JkiQ1zcArSZKkphl4JUmS1DQDryRJkppm4JUkSVLTDLySJElqmoFXkiRJTTPwSpIkqWkGXkmSJDXNwCtJkqSmGXglSZLUNAOvJEmSmmbglSRJUtMMvJIkSWqagVeSJElNM/BKkiSpaQZeSZIkNW1JBN4kr06yLMnNSc5N8thZ6j8/ybf7+hcnecbQurskeVdf/qskVyc5Icn9Fv9MJEmStNRMPfAm2RM4CngbsANwEXB6ks3H1H88cCJwLLA9cApwSpLt+ip37/dzaP/zOcDDgc8u3llIkiRpqZp64AVeB3yoqo6rqkuBfYAbgb3H1N8POK2qjqyqy6rqLcDXgX0Bqur6qtqtqj5ZVd+pqv/br9sxyQMW/3QkSZK0lEw18CZZH9gROGNQVlW398s7j9ls5+H6vdNXUh9gE6CAX4xpxwZJNh7cgI3mdAKSJEla8qbdw7spsC5wzUj5NcCWY7bZcj71k9wVeBdwYlWtGLPPg4Drh25XztpySZIkrRGmHXgXVZK7AJ8EArxyJVUPp+sFHty2WvzWSZIkaXVYb8rHvw64DdhipHwLYPmYbZbPpf5Q2H0g8NSV9O5SVbcAtwxtO5e2S5IkaQ0w1R7eqroVuADYdVCWZJ1++Zwxm50zXL+323D9obC7LfC0qvrpBJstSZKkNci0e3ihm5Ls+CTnA+cB+wP3AI4DSHICcFVVHdTXfy9wVpIDgFOBvYCdgFf09e8C/DvdlGR/CqybZDC+92d9yJYkSdJaYuqBt6pOSrIZ8Ha6C88uBHavqsGFaQ8Abh+qf3aSFwCHAe8ELgf2qKpv9VXuDzyr//3CkcM9BThz8mchSZKkpWrqgRegqo4BjhmzbpcZyk4GTh5TfxndRWqSJElS27M0SJIkSQZeSZIkNc3AK0mSpKYZeCVJktQ0A68kSZKaZuCVJElS0wy8kiRJapqBV5IkSU0z8EqSJKlpBl5JkiQ1zcArSZKkphl4JUmS1DQDryRJkppm4JUkSVLTDLySJElqmoFXkiRJTTPwSpIkqWkGXkmSJDXNwCtJkqSmGXglSZLUNAOvJEmSmmbglSRJUtMMvJIkSWqagVeSJElNM/BKkiSpaQZeSZIkNc3AK0mSpKYZeCVJktQ0A68kSZKaZuCVJElS0wy8kiRJapqBV5IkSU0z8EqSJKlpBl5JkiQ1zcArSZKkphl4JUmS1DQDryRJkppm4JUkSVLTDLySJElqmoFXkiRJTVtv2g2QJElrl20OPHXaTdAiWXbEM6fdhBnZwytJkqSmGXglSZLUNAOvJEmSmmbglSRJUtMMvJIkSWqagVeSJElNM/BKkiSpaQZeSZIkNc3AK0mSpKYZeCVJktQ0A68kSZKaZuCVJElS0wy8kiRJapqBV5IkSU0z8EqSJKlpBl5JkiQ1zcArSZKkphl4JUmS1DQDryRJkppm4JUkSVLTDLySJElqmoFXkiRJTTPwSpIkqWkGXkmSJDXNwCtJkqSmGXglSZLUNAOvJEmSmmbglSRJUtMMvJIkSWqagVeSJElNM/BKkiSpaQZeSZIkNc3AK0mSpKYZeCVJktQ0A68kSZKaZuCVJElS0wy8kiRJapqBV5IkSU1bEoE3yauTLEtyc5Jzkzx2lvrPT/Ltvv7FSZ4xsj5J3p7kf5LclOSMJNsu7llIkiRpKZp64E2yJ3AU8DZgB+Ai4PQkm4+p/3jgROBYYHvgFOCUJNsNVfs74LXAPsDjgF/1+7zrIp2GJEmSlqipB17gdcCHquq4qrqULqTeCOw9pv5+wGlVdWRVXVZVbwG+DuwLXe8usD9wWFV9pqq+CbwYuB+wx6KeiSRJkpac9aZ58CTrAzsChw/Kqur2JGcAO4/ZbGe6HuFhp/O7MPsgYEvgjKF9Xp/k3H7bT8zQjg2ADYaKNgJYsWLFPM5m1dx+y42r7VhavVbn82iYz6k2+XzSpE3jOeXzqV2r8/k0n2NNNfACmwLrAteMlF8D/N6YbbYcU3/LofXMUmfUQcDBo4Vbb731mOrS3G1y9LRboJb4fNKk+ZzSJE3p+bQRsNL0O+3Au1Qczp17je8N/GwKbVkbbARcCWwF/HLKbdGaz+eTJsnnkybN59Ti2gi4erZK0w681wG3AVuMlG8BLB+zzfJZ6i8fKvufkToXzrTDqroFuGWkeDqfG64FumHWAPyyqryftUp8PmmSfD5p0nxOLbo53adTvWitqm4FLgB2HZQlWadfPmfMZucM1+/tNlT/h3Shd3ifG9PN1jBun5IkSWrUtHt4oRtKcHyS84Hz6GZYuAdwHECSE4Crquqgvv57gbOSHACcCuwF7AS8AqCqKsnRwJuTXE4XgA+l6+4+ZfWckiRJkpaKqQfeqjopyWbA2+kuKrsQ2L2qBhedPQC4faj+2UleABwGvBO4HNijqr41tNu/pwvNHwTuCXyl3+fNi3s2mqNb6OZdHh1GIi2EzydNks8nTZrPqSUgVTXtNkiSJEmLZil88YQkSZK0aAy8kiRJapqBV5IkSU0z8GqNlqSS7DHtdmg65vr4Jzk0yQdXQ5NW1oZ9kvzHNNugpSPJIUkunHY7pLWFgVd3kuSjfZCoJLcm+V6StyaZdVaPJNv02z1mNTRVi6C1xz/JlsB+wDtGy5O8tz+/m5Nck+SrSV6Z5O5D9ZYN3R+/SvL1JM8fWv/RJKfMcNxd+m3u2Rd9BNghyZMW4zyXkiRbJ/lIkqv759CP+vv6PiP1zuzvo71GyvdPsmxo+aV9vdNG6t2zL9+lX/6D/njPGqn33P4x3m7CpzonY96YvZs7zymvRTb0tzzudsi02zgw8tpzY5KLk7y8X/fRWc5j2ZSbv+QYeDXOacB9gW2BfwAOAf52dTYgyV1W5/F0By09/i8Hzq6qHw3t+8HAN4A/Bt4IbA/sTDel4Z8CTxvZx1vp7o/tga8BJyV5/Hwa0X/Rzr8Br13YaawZ+vv2fLrnzl8CDwX2of9CoST3HtnkZuCwOTzevwGeluQp4ypU1UV0U1x+cBCuk2wOvB84eGT6ypWdQ+byBm9VVNUNVfXTxTyGZnTfodv+dN/SNVz27vnsbDH+TyVZf2hx8NqzHfAx4ENJnk73Jn643QAvG1r+X5Nu15rOwKtxbqmq5VX1o6r6F+AM4C+SrEjyvOGKSfboe742ovuiD4Bv9O8yz+zrrNP3El6Z5JYkFybZfWgfg57BPZOcleRm4IX9ur2TXNJv9z9Jjhlp66ZJPt2/A758tHdHC9LS478XMDqU4J/pAtROVfXJqrqsqn5QVZ+pqmfOUP+X/f3xXeDVwE3An8353vyd/wCeleRuC9h2TfFPwK3AH1fVWVX146r6L7o3EfdnpKcdOJFuvvS/nmW/v6LrJT9ilnqHAz/u2wHwAbr52scGmfyuN/7pSS6gmy/1iZmh9z7J0YPndb98ZpL3Jfn7JD9Lsny4l3Cop+3Twz1vGRnSMDhWkjem+7ThF/3fzHpJjuz3fWWSl420Z+skn+zr/yzJZ5JsM8t9tNbq/46XV9Vy4Pqu6A5leyW5LN0nAt9O8qrBtuNep5I8MMl/JPl5/1p4SZJnDG335CTnDb2GHTH8hqp/Dh3TP7euA04favLgtecHVfUu4GfAblV1/Ui7AX4xVPaTRbwb10gGXs3VTXRfAPIJuneRw14G/HtV/RJ4bF/2NLp3mc/pl/cDDgBeD/w+3R/0Z5NsO7KvI+i+Te8RwOlJXkn3j+uDwKOBZwHfG9nmYOCT/X4/B3w8d+5F0qpZIx///ucj6Xoc6cvuQ9ez+09V9auZTrZWMkF5Vf0G+DWw/rg6K3E+3Rf+PG4B2y55/f39J8A/V9VNw+v6f8ofB/ZMkqFVK+hC8FuT3GOWQxwCPHr0TdfIcW4DXgI8O8m/9e15aV8+myOAA+mef9+cQ/2Bl9AF8scBf0d3Lrv16wY9bYPet5X1vD0VuB/wR8Dr6L6s4D+Bn/f7fj/wgSRbwW97F08Hfgk8CXgCcANwWu7YS6g5SPJCuk8I3kT3HHgjcGiSl4xUvcPrFN1r1AZ0j9ujgTfQPQ4kuT/d69LXgD8AXgn8f8CbR/b5Ero3ik+g+0RktG3rJHkucK++nuarqrx5u8MN+ChwSv976MLLzcCRdIHmN8B9+/Wb0/3zf3K/vA1QwGNG9nkV8MaRsvPoQsfwdvvNsN1hK2lrAYcOLd+jL9t92vfjmnpr6fEHHtMvbz1U53F92Z+P7Os6un9SNwDvGipfBuzf/74+cFC//TNH76+R/e3S17vnSPnPgJdM+3FepOfO4L7dY8z6v+nXb94vnwkcTRcWlgFv6cv3B5YNbfdSut4r6Hpwv0P3xuGe/f52meFYh/fr/m4O7R48Vs8e97cwVHY0cObQ8pnAl2d4bh8x8jzdY6TOIcCFI8daBqwzVPZt4EtDy+v2z8+9+uW/6utkqM76wI10PexTf04s5dvw86pf/h7wlyN13kw3JArGv059k27IzEzHeMcMj9Gr6N6krDP0HPr6DNsuo/u04Qa619kCfgo8dIa6Y//uvHU3e3g1zp8muYEu6PwXcBJwSFWdB1xC924UuhfcHwFfGrejJBvT9Vp8dWTVV+neIQ8b7onbvN/uv2dp6297YqrrsVtBF8S0cK08/oOhA3P5WvHH0gXkS+gC2LB39ffHjXS9NwdW1alz2OdMbgLuPmutNVtmr/I7VXUL3VjF1yfZdJbq7wI2A/Yee/BkQ2BPusdrPhcJnj97lRmN9gb/Dwt7Dbqkqm4fWr4GuHiwUF0v9U+H9v0HdGOkf5nkhv45+jPgrsBDFnD8tVb/6cJDgGMH92V/f76ZO9+Xo8+T9wFvTnfR69uS/P7QukcA51SfSntfBTYEthoqu2BM046ke116KnAu8DdVNfopl+bAwKtxvkj3R7YtcLeqekn97uPfD9O9M4buY7rjRv6YV8XwR8w3ja11R78eWS58bq+qVh7/6/qf9xpa/72+zsPvsFE3Ru57Y447+KezFXCv6sbSDawANplhm3sCt3HHcwK4N9Dq+LrBfTv6RmbgEXQfz890/h+je/M0+lHvHVTVL+h6bw9m/BuHI+ne5Dye7kK3F8/W8N7oY3U7dw7vM12kNKnXoJn2s7J9b0gXlB4zcnsY3QWSmrsN+59/zR3vy+2APxype4fnSVV9GHgw8K90QxrOT/KaeR5/xuFVwHVV9b2q+jLwfOB9SR45z30LQ4HG+1X/R/bj6sYsDvsY8MAkr6UbH3n80LrB2KJ1BwVVtQK4mm5s0rAnAJeOa0B1Y0KX4dQ909DK4/99ukD6238Q1V0Z/3+AfecwZnRg8E9n+Qzh/jvAo5KM9grvAPywqn4bWJI8hK737RvzPI81wtB9+6rRC/PSTQ/3QuCkmd4g9T2bB9GNcdxmlkP9I10Y3W90RT929uV0w0YuogvQRye572jdOfgJv7sCfuAxC9jPrxn6m5igr9O9Kb22f34O365fhOM1q6quoXudevAM9+UP57D9FVX1/qp6Dt3MNoOLMC8Ddh4Zt/4EuiENV86zjVfQfdp2+Hy2U8fAq3mrqp8Dn6LrRfl8VQ3/0V5L10O2e5Itkgx6vo4E3tBf3frwJEfQ/eN47yyHOwQ4IMlrk2ybZIcFvHPWBK1Jj38fos4Anjiy6lV0Y0DP79v0iL5dfwX8Hl3P7Fx9nK7X7YQkOyZ5aJK96cah/sNI3ScBP6iq789j/2uafemGhJye5I/6WQR2pwvCV9FdEDSjfpjIucD/XtkBqupmuh7eO0zx1g+fORY4sqq+1he/h+6N1UK+eOQLwE5JXtw//95G1+M3X8uAXdPN/Xyv2SrPw8fpPsX4TJInJXlQuhkn3je4sE3zcjBwUP9687Akj07ysiSvW9lG/ewKf9Lf/zsAT6ELutDNCLM18I9Jfi/Js+kuRjxqZPjKXL0X+LMkOy1g27WagVcLdSzdxREfGS7sewNfS/cP62rgM/2q9wFH0QWAi4HdgWdV1eUrO0hVHU8XHF5FN7byP+l6NDRda9Lj/2G6qYZ++3rXB87t6cLw4cBFdOPyXkM3fdVb5rrz/iP2J9F91P1Z4EK6++B1dFNiDftL4EPzbP8apX9MdwJ+QDd7xvfpwuYXgZ2r6mez7OINdL3gszm+P8awo+mmmjpkqD230w29eeo8hjYMtj0dOJRufuavARsBJ8xnH70DgN2AK5hg735V3Ug3M8CP6d6EXkb3t3lXuk82NA/90ISX0z1fLgbOohu+NVsP77p0MzVcRjeH+XfpXrOoqquAZ9BdI3AR3UwbxwKHLbCNlwKfp5tNQvOQyQ2909okyYvoek7uV92E+lqLrEmPf/9R4rnAe6rqxCm241F0PYYP8+NmSVq97OHVvCS5ez8O8UDgA0s97Giy1sTHvx8v+gq6IQzTdF/gxYZdSVr97OHVvKT7BqE30U1D9eyqumG6LdLq5OMvSVoTGXglSZLUNIc0SJIkqWkGXkmSJDXNwCtJkqSmGXglSZLUNAOvJEmSmmbglSRJUtMMvJIkSWqagVeSJElNM/BKkiSpaf8PnRRVCfvUDDwAAAAASUVORK5CYII=\n",
      "text/plain": "<Figure size 800x600 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light",
      "transient": {}
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(8,6),dpi=100)\n",
    "\n",
    "x_arr = [1,2,3,4]\n",
    "x_name = [\"Pytorch\",\"Pytorch(GPU)\",\"ONNX runtime\",\"TersorRT\"]\n",
    "infer = [infer_torch_timer.totaltime,infer_torch_gpu_timer.totaltime,infer_onnx_timer.totaltime,infer_trt_timer.totaltime]\n",
    "plt.bar(x_arr,infer)\n",
    "plt.xticks(x_arr,x_name)\n",
    "plt.title('inference time')\n",
    "plt.ylabel('time (s)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorRT optimization\n",
    "\n",
    "1. Layer & Tensor Fusion\n",
    "    * 將 layer fusion 與移除沒用到的 layer\n",
    "    * ![Imgur](https://i.imgur.com/MI07Wn0.png)\n",
    "    * 如上圖中紅線匡選部份進行水平與垂直 fuse, 並且將 concat layer 給移除\n",
    "2. Weight &Activation Precision Calibration\n",
    "3. Kernel Auto-Tuning\n",
    "4. Dynamic Tensor Memory\n",
    "5. Multi-Stream Execution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### reference and issues:\n",
    "\n",
    "* issue: **AttributeError: 'tensorrt.tensorrt.Builder' object has no attribute 'max_workspace_size'** \n",
    "    * error when nvidia-tensorrt is 8.x.x.x version\n",
    "    * sol: remove nvidia-tensorrt 8.x.x.x and install 7.2.\n",
    "    * ref: [(github issue link)](https://github.com/NVIDIA-AI-IOT/torch2trt/issues/557)\n",
    "* issue: **AttributeError: module 'google.protobuf.descriptor' has no attribute '_internal_create_key**\n",
    "    * error when ```protobuf``` version is  not equal to ```protoc```\n",
    "        * show ```protobuf``` version: ```pip3 show protobuf```\n",
    "        * show ```protoc``` version: ```protoc --version```\n",
    "    * sol: \n",
    "        1. update protobuf version:  ```pip3 install --upgrade protobuf``` \n",
    "        2. install ```protoc``` if not exist:\n",
    "            1. ```wget https://github.com/protocolbuffers/protobuf/releases/download/v3.17.3/protoc-3.17.3-linux-x86_64.zip```\n",
    "            2. ```unzip protoc-3.17.3-linux-x86_64.zip```\n",
    "            3. ```vim ~/.bashprofile``` and paste ```PATH=$PATH:/home/YOUR_HOST_NAME/bin```\n",
    "            4. ```source ~/.bashprofile``` and ```echo $PATH``` to check whether ```/home/YOUR_HOST_NAME/bin``` exist or not.\n",
    "    * ref: [stackoverflow linke](https://stackoverflow.com/questions/61922334/how-to-solve-attributeerror-module-google-protobuf-descriptor-has-no-attribu)\n",
    "* issue: disable layer fusion 比對前後 perfermance?\n",
    "    * 根據 [How to disable graph optimization(layer fusion)?](https://forums.developer.nvidia.com/t/how-to-disable-graph-optimization-layer-fusion/146343) 表示目前不提供, 但根據 [Disable layer fusion optimizations? ](https://github.com/NVIDIA/TensorRT/issues/252) 提到有方法可以間接達成(```network.mark_output(layer)```), 但會額外造成 [latency](https://github.com/NVIDIA/TensorRT/pull/495)\n",
    "* [Imagenet label](https://gist.githubusercontent.com/yrevar/942d3a0ac09ec9e5eb3a/raw/238f720ff059c1f82f368259d1ca4ffa5dd8f9f5/imagenet1000_clsidx_to_labels.txt)\n",
    "* [NVIDIA onnx to tensorRT](https://developer.nvidia.com/blog/speeding-up-deep-learning-inference-using-tensorflow-onnx-and-tensorrt/)\n",
    "    * [zh-cn (注意中文文檔中 ```save_engine``` 中的 code 有錯)](https://developer.nvidia.com/zh-cn/blog/speeding-up-deep-learning-inference-using-tensorflow-onnx-and-tensorrt/)\n",
    "* [NVIDIA TensorRT Documentation](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#python_topics)\n",
    "* [NVIDIA TensorRT Quick Start Guide](https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html)\n",
    "* [Jetson Nano 运用TensorRT加速引擎 上篇](https://www.rs-online.com/designspark/nvidia-cudagpujetson-nano-tensorrt-cn)\n",
    "* [Jetson Nano 运用TensorRT加速引擎 下篇](https://www.rs-online.com/designspark/nvidia-cudagpujetson-nano-tensorrt-3-cn)\n",
    "* [Pytorch EXPORTING A MODEL FROM PYTORCH TO ONNX AND RUNNING IT USING ONNX RUNTIME](https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html)\n",
    "* [ONNX runtime document](https://onnxruntime.ai/python/tutorial.html)\n",
    "* [ONNX: Preventing Framework Lock in](https://towardsdatascience.com/onnx-preventing-framework-lock-in-9a798fb34c92)\n",
    "* [TensorRT 实现深度网络模型推理加速](https://on-demand.gputechconf.com/gtc-cn/2018/pdf/CH8212.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}